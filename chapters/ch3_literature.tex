\chapter{Literature Review}
\label{ch:literature}

The detection and characterization of binary microlensing events represents one of the outstanding observational challenges in time-domain astronomy. While the theoretical framework for binary lensing has been well established for decades, the practical problem of identifying binary events in real-time from ongoing survey data remains largely unsolved. This chapter surveys the literature on binary microlensing detection, from the scientific motivations that make this problem compelling (\cref{sec:binary_problem}) through traditional approaches (\cref{sec:traditional_detection}) and recent machine learning developments (\cref{sec:ml_microlensing}), culminating in the specific challenges posed by next-generation surveys such as the Nancy Grace Roman Space Telescope (\cref{sec:roman_preparation}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Binary Detection Problem}
\label{sec:binary_problem}

Gravitational microlensing events involving binary lenses occupy a privileged position in modern astrophysics because they provide unique access to physical parameters that remain inaccessible through other observational techniques. When a background source is lensed by a two-body system---whether a star with a planetary companion, a binary star, or even a pair of stellar remnants---the resulting light curve encodes the mass ratio $q$ and projected separation $s$ of the lens components \citep{Gaudi2012}. This sensitivity to mass, independent of luminosity, makes binary microlensing particularly valuable for studying populations of faint or dark objects that evade detection by traditional methods.

The scientific yield from binary microlensing has been substantial. Planetary companions produce characteristic caustic structures that manifest as brief, high-magnification anomalies superposed on otherwise smooth light curves. Since the first confirmed microlensing planet detection \citep{Bond2004}, the technique has revealed over 200 exoplanets with mass ratios and orbital separations that complement those accessible to transit and radial velocity surveys. Of particular importance is the sensitivity to cold, low-mass planets beyond the snow line---a population that is systematically underrepresented in transit surveys due to geometric selection effects \citep{Gaudi2012,Gould1992}. Microlensing is uniquely sensitive to companions down to Earth-mass at separations of order $1$--$5$~AU, a region largely inaccessible to other methods. Binary stellar systems similarly produce distinctive signatures that constrain the binary fraction and mass-ratio distribution of Galactic stellar populations, including systems containing white dwarfs, neutron stars, and black holes that would otherwise remain invisible \citep{Wyrzykowski2016,Sahu2022}.

Despite this scientific potential, the detection of binary microlensing events presents formidable observational challenges. The morphological diversity of binary light curves far exceeds that of single-lens events, encompassing a continuous spectrum from subtle perturbations to dramatic multi-peaked structures depending on the binary parameters and source trajectory \citep{Dominik1999}. The literature distinguishes between two primary detection channels, each presenting distinct challenges. In the \emph{planetary-caustic channel}, planetary caustics are closed curves of infinite magnification located away from the host star; when a source crosses or approaches these caustics, the light curve exhibits sharp spikes or U-shaped dips that can occur unpredictably at any point along the light curve's wings. In the \emph{high-magnification channel}, for events where the source passes very close to the lens ($u_0 \ll 1$), the source probes the central caustic induced by the planet near the host star, producing subtle perturbations at the very peak that can be degenerate with finite-source effects. Resonant caustics, which occur when the binary separation is close to the Einstein radius ($s \approx 1$), create particularly complex magnification patterns that may not obviously resemble classical binary signatures \citep{Cassan2008}. This diversity means that no single template describes all binary events.

The most insidious difficulty arises from the population of binary events that closely resemble point-source point-lens (PSPL) light curves. When the source trajectory passes far from the caustic structures---formally, when the impact parameter $u_0$ exceeds approximately $0.3$ Einstein radii---the binary perturbation becomes geometrically weak and may be effectively undetectable regardless of the sophistication of the analysis \citep{Dominik2007}. These events constitute a fundamental detection limit rather than an algorithmic failure: the magnification pattern of a wide-separation binary viewed at large impact parameter is mathematically indistinguishable from a PSPL curve within typical photometric uncertainties. This physical degeneracy has profound implications for population statistics, as it implies that any automated classifier must accept an irreducible misclassification rate for this subset of events. The detection problem is further complicated by well-known degeneracies in lens model parameters; the close-wide degeneracy implies that a planet at separation $s$ and one at $1/s$ produce nearly identical central caustic topologies \citep{Gaudi2012}.

The current state of binary event detection reflects these challenges. Ground-based surveys such as OGLE and KMTNet discover approximately 2,000 microlensing events annually, of which several hundred are ultimately classified as binary or planetary systems \citep{Mroz2020,Kim2018}. However, the classification process remains largely manual, relying on expert inspection of light curves that deviate from PSPL templates. As noted by \citet{Mroz2020}, ``there are no dedicated selection algorithms for binary-lens events''---a striking gap given the decades of theoretical and observational work in the field. Binary candidates are typically flagged when $\chi^2$ residuals to PSPL fits exceed some threshold, but this approach produces high false-positive rates from non-binary anomalies (stellar variability, blending artifacts, data reduction errors) and high false-negative rates for the weak-perturbation events described above.

This selection bias has tangible consequences for statistical inference. Surveys optimized for PSPL detection systematically undercount the binary population, particularly for configurations that produce subtle signatures. Planet occurrence rates derived from microlensing must account for this detection efficiency, but doing so requires precisely the automated, well-characterized selection function that currently does not exist \citep{Suzuki2016}. The problem grows increasingly acute as event detection rates climb: manual classification that sufficed for the OGLE-III era, with hundreds of events per year, already strains at OGLE-IV and KMTNet scales, and will become entirely untenable in the next generation of surveys.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Traditional Detection Approaches}
\label{sec:traditional_detection}

The standard approach to microlensing event classification begins with photometric monitoring and model fitting. As observations accumulate, each light curve is fit to the standard Paczyński model \citep{Paczynski1986} for a PSPL event using nonlinear optimization, typically minimizing $\chi^2$ between the model and data \citep{Udalski2015}. Events that are well-described by the PSPL model---achieving reduced $\chi^2$ values close to unity---are classified as single-lens events, while those showing significant residuals are flagged for further investigation. This approach is computationally tractable because the PSPL model has only three physical parameters (the Einstein crossing time $t_E$, the impact parameter $u_0$, and the time of closest approach $t_0$), and the optimization landscape is relatively well-behaved.

The limitations of $\chi^2$-based detection become apparent when binary classification is required. Binary lens models contain at minimum seven parameters: the three PSPL parameters plus the mass ratio $q$, the projected separation $s$, the angle $\alpha$ between the source trajectory and binary axis, and the source radius crossing time $\rho$ when finite-source effects are significant \citep{Dong2006}. The parameter space contains numerous local minima corresponding to different binary topologies, and the global optimum is often degenerate with qualitatively different physical configurations producing nearly identical light curves \citep{Gaudi2012}. Fitting a single binary model can require minutes of computation even with modern algorithms, and characterizing the full posterior distribution demands expensive sampling techniques such as Markov Chain Monte Carlo. When multiplied across thousands of ongoing events that must be refitted with each new observation, binary model fitting becomes prohibitively expensive for real-time survey operations.

Recognizing these limitations, several groups have developed feature-based classification schemes that extract summary statistics from light curves and apply machine learning classifiers to the resulting feature vectors. \citet{Wyrzykowski2015} applied random forest classification to OGLE-III light curves, achieving effective discrimination between microlensing events and other transient phenomena using 27 engineered features including baseline statistics, peak properties, and temporal characteristics. Their classifier achieved approximately $96.7\%$ accuracy, though with an initial false positive rate around $28\%$ that required a second-stage classifier to reduce. This work demonstrated that automated classification could achieve human-level performance for the microlensing-versus-non-microlensing distinction, but did not address the more challenging problem of distinguishing binary from single-lens events. Similarly, \citet{Godines2019} developed a random forest classifier for wide-field survey data using 47 statistical features and principal component analysis, achieving about $94\%$ accuracy in identifying microlensing candidates, but again focused on the detection problem rather than physical classification.

The work of \citet{Khakpash2021} represents the closest antecedent to binary-specific classification. Targeting the anticipated data characteristics of the Roman Space Telescope, Khakpash and collaborators designed fifteen features specifically intended to capture binary signatures: measures of peak smoothness that detect the sharp caustic-crossing features absent in PSPL events, symmetry metrics that flag the asymmetric structures produced by many binary configurations, and peak-counting statistics that identify multi-peaked light curves. Their random forest classifier achieved promising results on simulated Roman data, demonstrating that automated binary classification is feasible in principle.

However, feature-based approaches carry intrinsic limitations that become increasingly problematic at survey scale. The features must be designed by domain experts who anticipate the morphological signatures of binary events---but as discussed in \cref{sec:binary_problem}, the diversity of binary light curves is enormous, and features optimized for caustic-crossing events may fail entirely for cusp-approach or resonant configurations. The feature engineering process is inherently subjective, and there is no guarantee that hand-crafted features capture all the information relevant to classification. Moreover, features computed from partial light curves early in an event's evolution may behave differently from those computed at completion, a phenomenon noted as ``peak bias'' by \citet{Godines2019}, requiring careful calibration for real-time applications. These considerations motivate the exploration of deep learning approaches that learn directly from raw light curve data rather than engineered summaries.

The scaling problem provides the ultimate motivation for automation. During the OGLE-III era, when event rates were hundreds per year, expert astronomers could plausibly inspect each interesting light curve and make informed classification decisions. At OGLE-IV and KMTNet scales, with thousands of events annually, manual inspection remains possible but increasingly represents a bottleneck in the discovery pipeline. The situation will become untenable in the next decade. The Vera C.\ Rubin Observatory's Legacy Survey of Space and Time (LSST) is projected to discover approximately 20,000 microlensing events per year \citep{LSSTScienceBook,Abrams2023}, while the Roman Space Telescope's Galactic Bulge Time-Domain Survey will detect roughly 30,000 events over its mission lifetime \citep{Penny2019}. At these rates, classification must be automated, rapid, and well-characterized---requirements that current methods do not satisfy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neural Networks for Microlensing}
\label{sec:ml_microlensing}

The application of neural networks to microlensing classification represents a natural evolution from feature-based methods, motivated by the success of deep learning across astronomical domains. Neural networks learn hierarchical representations directly from raw data, potentially capturing patterns that would not be encoded by hand-crafted features. Several recent works have explored this direction with encouraging results, while also revealing architectural sensitivities that inform the present thesis.

\subsection{Pioneering Work: Separate Networks for Each Class}

The foundational work by \citet{Mroz2020} demonstrated dedicated neural network classification for microlensing events. Recognizing the morphological distinctiveness of different event types, Mróz and collaborators trained \emph{separate} networks for PSPL and binary classification rather than a single unified model. Their networks utilized a standard architecture comprising convolutional layers followed by fully connected dense layers, treating the light curve classification as a one-dimensional image recognition problem. The PSPL classifier achieved approximately $98\%$ accuracy on held-out test data, while the binary classifier reached $80$--$85\%$ accuracy depending on the test set composition. Importantly, this work demonstrated that neural networks could learn to recognize caustic-crossing signatures from flux time series without explicit feature engineering---the network learned to detect the characteristic shapes automatically.

The approach of treating the classes separately was a key insight: it mitigated the issue of class imbalance (far fewer binaries than singles in the training data) and allowed each network to specialize. However, the fragmented architecture requires careful calibration to combine outputs, and the relatively small training sets (on the order of $10^3$ events drawn from OGLE data) limited the complexity of models that could be effectively trained. The system was not a unified classifier but rather a two-step process, leaving open the question of how to integrate such classifiers into a real survey pipeline in a seamless way.

\subsection{Real-Time Classification Systems}

More recent work has addressed the real-time classification problem for active surveys. Recurrent neural network architectures, specifically Long Short-Term Memory (LSTM) networks, have been deployed for event filtering in ground-based survey alert streams. These systems process the time-series flux data sequentially, updating their classification assessment as new data points arrive. The recurrent architecture naturally handles variable-length sequences and provides the ``drip-feed'' capability that real-time applications require: early in an event, the classifier may be uncertain, but confidence typically improves as the light curve develops toward peak.

Such systems have achieved high accuracy (often exceeding $99\%$ precision) in distinguishing genuine microlensing events from instrumental artifacts and non-microlensing variability. A critical finding from this work is that classification confidence improves as additional observations accumulate---a property that any real-time classifier must exhibit. However, these systems have focused primarily on event detection (microlensing versus non-microlensing) rather than physical subclassification (PSPL versus binary), leaving the binary detection problem unaddressed.

\subsection{Architectural Considerations: RNNs versus Transformers}

Recent work on microlensing parameter inference provides cautionary perspective on architectural choices. Experiments with Gated Recurrent Unit (GRU) networks for regressing physical parameters from light curves revealed that GRUs can ``fail catastrophically'' when presented with irregularly sampled sequences typical of ground-based observations. The failure mode involved large, unpredictable errors that undermined the network's utility for scientific inference. The culprit was the RNN's difficulty in handling long gaps or non-uniform time steps, which can confuse the sequential state updates.

Switching to Transformer architectures with explicit positional encoding for observation times resolved the instability, yielding robust parameter estimates across a range of sampling patterns. Unlike RNNs, which process data sequentially, Transformers use self-attention mechanisms to process the entire sequence in parallel, computing weights that relate every point in the light curve to every other point. When observation timestamps are encoded directly, the Transformer can ``attend'' to anomalies and baselines simultaneously, regardless of their temporal separation. This architectural lesson is highly relevant: while LSTMs and GRUs can work well for smoothly sampled and moderately long sequences, they may struggle with either extremely long sequences or complex time sampling. For Roman's nearly uniform 15-minute cadence, however, LSTM architectures remain viable and computationally efficient.

The contrast between different architectures suggests a trade-off: Transformers offer flexibility and robustness to irregular sampling at the cost of higher computational complexity (attention scales as $O(N^2)$ with sequence length) and larger training data requirements. LSTMs, with their gated memory cells, provide a middle ground---capable of preserving information over long sequences while maintaining computational efficiency suitable for real-time deployment.

\subsection{Gaps in Current Approaches}

Despite these advances, several gaps in the literature motivate the present thesis:

\begin{enumerate}
    \item \textbf{No unified hierarchical classifier}: No existing work provides a single model that handles the complete taxonomy of microlensing events---Flat (no lensing), PSPL (single point lens), and Binary---in an integrated framework. The hierarchical structure is scientifically natural (one first asks whether magnification is present, then characterizes its source) but has not been implemented in published systems.
    
    \item \textbf{Limited training scale}: While \citet{Mroz2020} used hundreds of events and \citet{Khakpash2021} used thousands, the Roman survey will generate tens of thousands of events, suggesting that training sets of comparable or larger size may be necessary to capture the full diversity of the population.
    
    \item \textbf{No confidence evolution analysis}: No systematic analysis has characterized how classification confidence evolves as observations accumulate during an ongoing event. This ``early detection'' capability is scientifically critical: binary events require immediate follow-up to capture caustic-crossing features, and waiting until an event is complete defeats the purpose of real-time classification.
    
    \item \textbf{No physical detection limits}: No published work has systematically characterized the physical detection limits of neural network classifiers as a function of impact parameter $u_0$, mass ratio $q$, and other physical parameters. Understanding these limits is essential for interpreting classifier outputs and for designing survey strategies that account for unavoidable selection effects.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preparing for the Roman Space Telescope}
\label{sec:roman_preparation}

The Nancy Grace Roman Space Telescope, scheduled for launch in the late 2020s, will transform gravitational microlensing science through its dedicated Galactic Bulge Time-Domain Survey. The observational parameters of this survey define the technical requirements for any automated classification system, and the expected data volume makes automation not merely desirable but essential.

\subsection{Roman's Observational Parameters}

Roman will observe the Galactic bulge in multiple 72-day seasons, with a nominal cadence of 15 minutes between observations in the primary microlensing field \citep{Penny2019}. This cadence represents a substantial improvement over ground-based surveys, which typically achieve hourly sampling at best and are interrupted by weather, daylight, and seasonal visibility. At 15-minute cadence, a single Roman observing season will produce up to 6,912 photometric measurements per light curve---far more than the hundreds of points typical of ground-based coverage. This dense sampling ensures that caustic-crossing features, which may last only hours, will be resolved with multiple data points, providing the information content necessary for reliable classification.

The expected yield from Roman's microlensing survey is approximately 30,000 events over the mission lifetime, with roughly 1,400 expected to show detectable planetary signals \citep{Penny2019}. This represents an order-of-magnitude increase over the cumulative yield of all ground-based surveys to date. The total number of confirmed planetary microlensing events from ground-based history is approximately 200--300, a sample size orders of magnitude too small to train deep neural networks without severe overfitting. Many Roman events will be inaccessible to ground-based follow-up due to their faintness or their position in crowded stellar fields where ground-based resolution is inadequate. For these events, the Roman data alone must suffice for classification and characterization, placing a premium on automated analysis pipelines that can extract maximum information from the space-based photometry.

\subsection{The Real-Time Classification Imperative}

Real-time classification is not merely a convenience---it is essential for maximizing Roman's scientific return. Binary microlensing events containing planets require intensive follow-up observations to characterize the anomaly fully: high-cadence photometry during caustic crossings, and ideally multi-site coverage to ensure continuous monitoring. For ground-accessible events, this follow-up can be coordinated through networks such as the Microlensing Follow-Up Network ($\mu$FUN) and the Las Cumbres Observatory \citep{Gould2010,Street2018}. However, the decision to allocate follow-up resources must be made while the event is ongoing---often within hours of the first anomaly detection.

Classification systems that require complete light curves, or that achieve reliable discrimination only near event peak, cannot support this operational model. What is needed is a classifier that provides calibrated confidence estimates from partial data, enabling follow-up decisions to be made with quantified uncertainty early in an event's evolution. Furthermore, the classifier must be computationally efficient: with potentially hundreds of events active simultaneously during a Roman observing season, inference times must be measured in milliseconds rather than seconds.

\subsection{Contributions of This Thesis}

This thesis addresses the requirements outlined above through several specific contributions:

\begin{enumerate}
    \item \textbf{Million-scale training}: We develop a unified hierarchical classifier trained on over 1.3 million synthetic microlensing events, exceeding previous training set sizes by three orders of magnitude and providing the diversity necessary to generalize across the parameter space expected from Roman.
    
    \item \textbf{LSTM architecture}: We employ an LSTM architecture that naturally handles the sequential structure of time-series data, with explicit encoding of observation timestamps to accommodate variable cadence. The LSTM maintains a persistent hidden state that accumulates evidence across observations, preserving memory of brief caustic crossings that might occur tens of days before classification is performed.
    
    \item \textbf{Physical detection limits}: We systematically characterize classification performance as a function of the impact parameter $u_0$, identifying the physical boundary at $u_0 \approx 0.3$ beyond which binary events become intrinsically indistinguishable from PSPL events. This characterization provides the selection function necessary for interpreting classifier outputs in population studies.
    
    \item \textbf{Real-time capability}: We demonstrate inference times under one millisecond per event, enabling deployment in alert stream processing pipelines that must handle thousands of events simultaneously.
\end{enumerate}

The following chapters describe the methodology used to achieve these goals (\cref{ch:methodology}), present experimental results validating the approach (\cref{ch:results}), and discuss the implications for Roman survey operations and microlensing science more broadly (\cref{ch:discussion}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FIGURE PLACEHOLDERS - Add these figures when available
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Suggested figures for Chapter 3:
%
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/fig_literature_timeline.pdf}
%     \caption[Timeline of machine learning in microlensing.]{Timeline showing the evolution of machine learning approaches in microlensing, from feature-based random forests (2015) through dedicated neural networks (2020) to the hierarchical LSTM classifier developed in this thesis.}
%     \label{fig:ml_timeline}
% \end{figure}
%
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/fig_detection_comparison.pdf}
%     \caption[Comparison of detection approaches.]{Schematic comparison of traditional $\chi^2$-based detection (left), feature-based classification (center), and end-to-end neural network classification (right). The neural network approach eliminates the information loss inherent in feature engineering.}
%     \label{fig:detection_comparison}
% \end{figure}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%