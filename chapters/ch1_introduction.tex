\chapter{Introduction}
\label{ch:introduction}

\section{Motivation and Background}
\label{sec:motivation}

Modern astronomy is increasingly defined by the scale and velocity of data acquisition, pushing the boundaries of computational capacity and observational efficiency. Wide-field time-domain surveys have transformed the field from targeted observation to automated, rapid data processing across millions of stellar sources. Among the most powerful techniques for detecting exoplanets and probing the structure of the Galaxy is gravitational microlensing \citep{Einstein1936,Paczynski1986}, a phenomenon predicted by Einstein's general relativity in which a massive foreground object (the lens) temporarily magnifies the light of a background star (the source) as they align along the line of sight to Earth \citep{Gaudi2012}.

The elegance of gravitational microlensing lies in its mass-dependent nature: because the lensing effect depends only on mass and geometry rather than the luminosity of the lens, microlensing can detect planets around faint stars, free-floating planets unbound to any host, and stellar remnants that would remain invisible to traditional detection methods \citep{Mao1991,Gould1992}. The probability that any random star is sufficiently aligned to be lensed is extremely small---the optical depth toward the Galactic bulge is of order $10^{-6}$ \citep{Mao2012}---so millions of stars must be continuously monitored to detect even a handful of events. Over the past three decades, ground-based surveys such as the Optical Gravitational Lensing Experiment (OGLE) and the Microlensing Observations in Astrophysics (MOA) have monitored dense stellar fields in the Galactic bulge at cadences of minutes to hours, discovering over 200 exoplanets and characterizing the demographics of planetary systems throughout the Galaxy \citep{Udalski2015,Bond2017,Mroz2020}.

The majority of observed microlensing events are well described by the Point-Source Point-Lens (PSPL) model \citep{Paczynski1986}, which assumes both the lens and source are point masses. These events produce characteristic, symmetric, achromatic light curves described by only three parameters: the Einstein crossing time, the impact parameter, and the time of maximum magnification. However, the most scientifically valuable events are those caused by binary lenses---two-body systems such as a star-planet pair or a binary star system. Binary lens events produce complex magnification patterns with caustic structures, multiple peaks, and asymmetries that depend sensitively on the lens geometry and the source trajectory \citep{Gaudi2012,Dominik1999}. These deviations from the simple PSPL template are crucial for determining physical parameters such as planetary mass ratios and projected separations.

The primary observational challenge lies in the temporal ambiguity problem: during the initial rise of a microlensing event, binary lens systems often masquerade as simple PSPL events, only revealing their true nature when the source approaches or crosses caustic features in the lens plane. This degeneracy persists until the event is sufficiently developed, leading to critical delays in follow-up observations necessary to fully characterize the binary system. Current surveys discover approximately 2,000 events per year \citep{Mroz2020}---a number that human experts and traditional modeling approaches can manage through careful monitoring. However, the upcoming data deluge from the Vera C.~Rubin Observatory's Legacy Survey of Space and Time (\lsst) will increase this rate tenfold, discovering an estimated 20,000 microlensing events annually \citep{LSSTScienceBook,Abrams2023}. Simultaneously, the Nancy Grace \romantel\ Space Telescope, with its dedicated Galactic Bulge Time-Domain Survey, aims to detect on the order of $2 \times 10^4$ microlensing events over its mission lifetime \citep{Penny2019,Street2018}, providing complementary space-based observations with superior photometric precision and continuous monitoring capabilities. This enormous increase in event detection rates mandates a complete re-evaluation of current classification strategies. The future of microlensing science, particularly in the statistical characterization of exoplanet populations and Galactic structure, depends critically on developing automated systems capable of early, reliable classification.

\section{Research Problem}
\label{sec:problem}

The fundamental challenge addressed by this thesis is the computational inefficiency and latency of classifying binary microlensing events in the era of large-scale surveys. Traditional classification methods, typically relying on $\chi^2$ model fitting or manual inspection, suffer from several critical limitations. First, they are computationally intensive and scale poorly with the incoming data rates expected from \lsst\ and \romantel. Binary lens models have significantly more free parameters than PSPL models (typically seven or more versus three), and their complex parameter spaces contain numerous local minima, making optimization challenging and computationally expensive \citep{Dong2006}. Fitting a single binary lens model to a light curve can require seconds to minutes per evaluation, and characterizing a single event can take hours of computation when accounting for multiple binary topologies and the need for global optimization. When multiplied across thousands of ongoing events that must be updated with each new observation, the computational burden becomes prohibitive for real-time classification during survey operations.

Second, traditional methods lack a principled framework for handling partial observations. Most fitting procedures assume access to a complete or nearly complete light curve, yielding unreliable parameter estimates when applied to early-time data before the event has reached peak magnification or revealed its binary nature. The standard approach of fitting PSPL models to incomplete data and flagging large residuals works reasonably well for high-magnification events but performs poorly for events with subtle binary perturbations that only become apparent late in the observational sequence. This lack of early-warning capability is particularly problematic for planetary detections, where short-lived caustic crossings may occur unpredictably during any phase of the event and require immediate high-cadence follow-up to fully characterize.

Third, existing classification infrastructure is already severely strained by current survey data rates, and will be inadequate for \lsst-era operations. The current reliance on human-in-the-loop systems and computationally expensive template fitting is not sustainable for the upcoming ${\sim}10\times$ increase in event detection rate. Manual inspection of thousands of ongoing light curves is impractical; automated systems that can process incoming photometry in real time, assign reliable classification probabilities, and trigger follow-up observations based on scientifically motivated criteria are essential. The computational efficiency of any such system must be orders of magnitude better than current approaches to enable processing of alert streams within seconds of new observations arriving.

This thesis addresses the research problem: \emph{How can we design a machine-learning model that reliably distinguishes binary microlensing events from PSPL events using only partial, noisy light curves, enabling early classification and efficient resource allocation for follow-up observations?} This requires moving beyond traditional feature engineering toward a representation learning approach that can model the temporal dependencies of light curve data, process incomplete observations as they arrive, scale efficiently to tens of thousands of events, and provide reliable confidence estimates to guide operational decisions.

\section{Research Questions}
\label{sec:questions}

To make the research problem tractable and measurable, this thesis addresses the following specific research questions:

\begin{enumerate}
    \item \textbf{Can a CNN-GRU architecture effectively distinguish between flat, PSPL, and binary microlensing events from synthetic Roman-like light curves?} We investigate whether deep learning can learn representations that capture caustic crossing signatures without explicit feature engineering.
    
    \item \textbf{How does classification performance depend on the training data distribution?} We examine performance when training on events with clear caustic signatures (distinct distribution, $u_0 < 0.3$) versus more challenging realistic populations (general distribution, full parameter space).
    
    \item \textbf{What are the dominant error patterns and do they reflect physical limitations or algorithmic failures?} We analyze the confusion matrix to determine whether misclassifications occur in regimes where binary events are genuinely indistinguishable from single lenses.
    
    \item \textbf{Does the model provide well-calibrated probability estimates suitable for operational decision-making?} We assess whether predicted confidence levels accurately reflect true classification accuracy, which is essential for prioritizing events in real surveys.
\end{enumerate}

These questions provide a structured framework for evaluating the feasibility of machine learning approaches to binary microlensing classification in preparation for Roman operations.

\section{Objectives}
\label{sec:objectives}

To address these research questions, this thesis pursues the following concrete objectives:

\begin{enumerate}
    \item \textbf{Synthetic Data Generation:} Generate a large-scale synthetic dataset of 900,000 high-fidelity light curves (600,000 training, 300,000 test) using VBBinaryLensing with Roman-like observational parameters (15-minute cadence, 72-day seasons). Implement a dual-distribution sampling strategy: a distinct distribution emphasizing clear caustic signatures ($u_0 < 0.3$, $s \in [0.8, 1.2]$) and a general distribution representing the expected Roman event population across the full parameter space.
    
    \item \textbf{Architecture Development:} Design and implement a CNN-GRU hierarchical classifier specifically tailored for microlensing light curves. The architecture combines convolutional layers for local feature extraction (caustic crossings) with GRU layers for long-term temporal memory, using a two-stage hierarchical structure (Flat vs Non-Flat, then PSPL vs Binary) to reflect the logical classification problem.
    
    \item \textbf{Performance Evaluation:} Evaluate classification accuracy, precision, recall, F1-scores, and ROC-AUC on held-out test sets from both distinct and general distributions. Analyze confusion matrices to identify error patterns and assess whether performance degradation reflects genuine physical detection limits.
    
    \item \textbf{Model Interpretability:} Examine calibration curves to verify that predicted probabilities accurately reflect true classification accuracy. Analyze example probability evolution trajectories showing how classifications update as observations arrive during the 72-day season.
\end{enumerate}

Together, these objectives aim to demonstrate the feasibility of deep learning for binary microlensing classification and establish baseline performance benchmarks for future work.

\section{Contributions}
\label{sec:contributions}

This thesis makes the following contributions to automated microlensing classification:

\begin{enumerate}
    \item \textbf{Large-Scale Synthetic Dataset:} We generate 900,000 high-fidelity synthetic light curves using VBBinaryLensing with Roman-like observational parameters (15-minute cadence, 72-day seasons). The dataset includes 600,000 training events and 300,000 test events with equal representation of three classes (flat, PSPL, binary). A dual-distribution strategy—distinct events with clear caustic signatures ($u_0 < 0.3$) and general events representing the full parameter space—enables controlled evaluation of generalization.
    
    \item \textbf{Proof-of-Concept CNN-GRU Classifier:} We demonstrate that a compact hierarchical architecture (~33,541 parameters) can achieve 99.65\% accuracy on events with clear binary signatures, validating that deep learning can recognize caustic crossing patterns without hand-engineered features. The hierarchical structure (Flat vs Non-Flat, then PSPL vs Binary) naturally decomposes the classification problem.
    
    \item \textbf{Calibration Assessment:} We demonstrate that the model produces well-calibrated probability estimates where predicted confidence levels accurately reflect true classification accuracy. This calibration is essential for operational use, enabling reliable event prioritization based on model outputs.
    
    \item \textbf{Open-Source Implementation:} All code for data generation, model training, and evaluation is released to facilitate reproduction and extension of this work.
\end{enumerate}

These contributions establish the feasibility of deep learning approaches for binary microlensing classification and provide a foundation for future development of operational systems for Roman and other next-generation surveys.

\section{Thesis Organization}
\label{sec:organization}

The remainder of this thesis is organized as follows:

\medskip  % Space before first item

\noindent\textbf{Chapter 2: Theoretical Background} reviews the physics of gravitational microlensing, deriving the lens equation for both single (PSPL) and binary lenses. We explain the origin of caustic structures in binary systems and describe how caustic crossings produce the distinctive light curve features that distinguish binary from single-lens events. The chapter introduces the mathematical framework needed to understand the synthetic data generation and classification problem.

\medskip  % Space between items

\noindent\textbf{Chapter 3: Literature Review} surveys related work in three areas: traditional microlensing classification methods ($\chi^2$ fitting, Bayesian inference), machine learning applications to astronomical time series (supernova classification, exoplanet detection), and previous attempts to apply neural networks to microlensing. We identify the research gap: the absence of end-to-end deep learning classifiers that operate directly on raw photometry without requiring feature engineering or model fitting.

\medskip  % Space between items

\noindent\textbf{Chapter 4: Methodology} describes the technical approach. We explain the synthetic data generation using VBBinaryLensing to create 900,000 Roman-like light curves, explain the dual-distribution sampling strategy (distinct vs general), present the CNN-GRU hierarchical architecture, and document the training procedures and evaluation protocols.

\medskip  % Space between items

\noindent\textbf{Chapter 5: Results} presents classification performance on distinct and general test sets. We report overall accuracy, per-class metrics, confusion matrices, and calibration curves. Analysis of error patterns reveals that Binary$\to$PSPL misclassifications dominate, consistent with physical degeneracies at high impact parameters. Probability evolution examples demonstrate how classifications update as observations arrive.

\medskip  % Space between items

\noindent\textbf{Chapter 6: Discussion} interprets the results in context, examining whether performance limitations reflect algorithmic failures or genuine physical detection thresholds. We discuss implications for Roman operations, analyze the model's strengths and limitations, and contextualize findings relative to previous work in astronomical machine learning.

\medskip  % Space between items

\noindent\textbf{Chapter 7: Conclusions and Future Work} summarizes key findings, reviews how the work addresses each research question, and outlines directions for future research including application to real survey data, extension to more complex lens systems, and integration with Roman alert systems.

\medskip  % Space between items

\noindent\textbf{Appendices} provide supporting material including derivations of magnification formulas, complete architecture specifications, and additional performance metrics.

\medskip

This organizational structure ensures that readers can follow the logical progression from motivation through methods to conclusions, while also enabling experts to navigate directly to chapters of specific interest. Figures and tables throughout illustrate key concepts, simulation outcomes, and classifier performance. A comprehensive bibliography provides context within the broader literature of microlensing physics, exoplanet detection, and machine learning applications in astronomy.