\chapter{Introduction}
\label{ch:introduction}

\section{Motivation and Background}
\label{sec:motivation}

Modern astronomy is increasingly defined by the scale and velocity of data acquisition, pushing the boundaries of computational capacity and observational efficiency. Wide-field time-domain surveys have transformed the field from targeted observation to automated, rapid data processing across millions of stellar sources. Among the most powerful techniques for detecting exoplanets and probing the structure of the Galaxy is gravitational microlensing \citep{Einstein1936,Paczynski1986}, a phenomenon predicted by Einstein's general relativity in which a massive foreground object (the lens) temporarily magnifies the light of a background star (the source) as they align along the line of sight to Earth \citep{Gaudi2012}.

The elegance of gravitational microlensing lies in its democratic nature: because the lensing effect depends only on mass and geometry rather than the luminosity of the lens, microlensing can detect planets around faint stars, free-floating planets unbound to any host, and stellar remnants that would remain invisible to traditional detection methods \citep{Mao1991,Gould1992}. The probability that any random star is sufficiently aligned to be lensed is extremely small---the optical depth toward the Galactic bulge is of order $10^{-6}$ \citep{Mao2012}---so millions of stars must be continuously monitored to detect even a handful of events. Over the past three decades, ground-based surveys such as the Optical Gravitational Lensing Experiment (OGLE) and the Microlensing Observations in Astrophysics (MOA) have monitored dense stellar fields in the Galactic bulge at cadences of minutes to hours, discovering over 200 exoplanets and characterizing the demographics of planetary systems throughout the Galaxy \citep{Udalski2015,Bond2017,Mroz2020}.

Historically, the majority of observed microlensing events are well described by the Point-Source Point-Lens (PSPL) model \citep{Paczynski1986}, which assumes both the lens and source are point masses. These events produce characteristic, symmetric, achromatic light curves described by only three parameters: the Einstein crossing time, the impact parameter, and the time of maximum magnification. However, the most scientifically valuable events are those caused by binary lenses---two-body systems such as a star-planet pair or a binary star system. Binary lens events produce complex magnification patterns with caustic structures, multiple peaks, and asymmetries that depend sensitively on the lens geometry and the source trajectory \citep{Gaudi2012,Dominik1999}. These deviations from the simple PSPL template are crucial for determining physical parameters such as planetary mass ratios and projected separations.

The primary observational challenge lies in the temporal ambiguity problem: during the initial rise of a microlensing event, binary lens systems often masquerade as simple PSPL events, only revealing their true nature when the source approaches or crosses caustic features in the lens plane. This degeneracy persists until the event is sufficiently developed, leading to critical delays in follow-up observations necessary to fully characterize the binary system. Current surveys discover approximately 2,000 events per year \citep{Mroz2020}---a number that human experts and traditional modeling approaches can manage through careful monitoring. However, the upcoming data deluge from the Vera C.~Rubin Observatory's Legacy Survey of Space and Time (\lsst) will increase this rate tenfold, discovering an estimated 20,000 microlensing events annually \citep{LSSTScienceBook,Abrams2023}. Simultaneously, the Nancy Grace \romantel\ Space Telescope, with its dedicated Galactic Bulge Time-Domain Survey, aims to detect on the order of $2 \times 10^4$ microlensing events over its mission lifetime \citep{Penny2019,Street2018}, providing complementary space-based observations with superior photometric precision and continuous monitoring capabilities. This enormous increase in event detection rates mandates a complete re-evaluation of current classification strategies. The future of microlensing science, particularly in the statistical characterization of exoplanet populations and Galactic structure, depends critically on developing automated systems capable of early, accurate classification.

\section{Research Problem}
\label{sec:problem}

The fundamental challenge addressed by this thesis is the inefficiency and latency of classifying binary microlensing events in the era of large-scale surveys. Traditional classification methods, typically relying on $\chi^2$ model fitting or manual inspection, suffer from several critical limitations. First, they are computationally intensive and scale poorly with the incoming data rates expected from \lsst\ and \romantel. Binary lens models have significantly more free parameters than PSPL models (typically seven or more versus three), and their complex parameter spaces contain numerous local minima, making optimization challenging and computationally expensive \citep{Dong2006}. Fitting a single binary lens model to a light curve can require seconds to minutes per evaluation, and characterizing a single event can take hours of computation when accounting for multiple binary topologies and the need for global optimization. When multiplied across thousands of ongoing events that must be updated with each new observation, the computational burden becomes prohibitive for real-time classification during survey operations.

Second, and more critically, traditional methods require a high degree of observation completeness---meaning the light curve must be substantially developed, often past the peak magnification, to reveal the subtle features indicative of a binary system. The PSPL model can accurately fit the initial brightening phase of many binary events that have not yet revealed their characteristic caustic features \citep{Gaudi2008}. By the time these distinguishing features become apparent, the optimal window for additional observations may have passed, particularly for short-duration events or events discovered near their peaks. If a binary event is classified too late, critical high-magnification features may be missed, rendering the determination of planetary mass ratios and separations highly uncertain. Furthermore, late classification prevents timely triggering of global follow-up networks, which may have observational windows lasting only hours to days \citep{Yee2012,Beaulieu2006}, leading to suboptimal data acquisition and reduced scientific yield.

The impending arrival of \lsst\ data exacerbates all of these challenges. The survey's unprecedented depth and cadence will discover events at earlier phases and in greater numbers than current surveys, while its wide field of view and multiple filters will generate heterogeneous light curves with varying sampling rates and photometric precision \citep{Ivezic2019}. Existing classification infrastructure, already strained by current survey data rates, will be completely inadequate for \lsst-era operations. The current reliance on human-in-the-loop systems and computationally expensive template fitting is not sustainable for the upcoming $\sim\!10\times$ increase in event detection rate.

This thesis addresses the research problem: \emph{How can we design a machine-learning model that reliably distinguishes binary microlensing events from PSPL events using only partial, noisy light curves, enabling early classification and efficient resource allocation for follow-up observations?} This requires moving beyond traditional feature engineering toward a representation learning approach that can model the temporal dependencies of light curve data, process incomplete observations as they arrive, scale efficiently to tens of thousands of events, and provide reliable confidence estimates to guide operational decisions.

\section{Research Questions}
\label{sec:questions}

To make the research problem tractable and measurable, this thesis addresses the following specific research questions:

\begin{enumerate}
    \item \textbf{Can a one-dimensional Convolutional Neural Network (1D \cnn), specifically designed for time-series data, effectively distinguish between PSPL and binary microlensing events using incomplete light curves?} We investigate whether deep learning architectures can learn representations that capture the subtle deviations characteristic of binary lenses without explicit feature engineering.
    
    \item \textbf{How does classification accuracy correlate with observation completeness, and at what minimum completeness level can high-confidence classification ($\geq\!90\%$ accuracy) be achieved?} We quantify the trade-off between timeliness and reliability by evaluating performance across observation fractions ranging from 10\% to 100\% of the full event duration, determining the earliest point at which reliable identification becomes possible.
    
    \item \textbf{Does the proposed TimeDistributed architecture enhance the model's ability to extract time-varying features and provide a performance advantage over traditional machine learning approaches and standard single-shot \cnn\ models?} We compare the TimeDistributed design, which processes sequential observations as they arrive, against baseline methods including feature-based random forests and standard \cnn\ architectures.
    
    \item \textbf{How sensitive is classification performance to survey cadence and photometric noise, and can a model trained on synthetic data generalize to real survey observations?} By varying sampling intervals and noise levels to mimic OGLE, MOA, and \lsst\ characteristics, we evaluate robustness across different observing strategies. We test generalization by applying the \cnn\ to real OGLE and MOA data and benchmark against traditional methods using metrics such as precision, recall, and area under the receiver operating characteristic curve (AUC).
\end{enumerate}

These questions provide a structured framework for the experiments described in subsequent chapters. By answering them systematically, we aim to determine both the feasibility and the practical limitations of early microlensing classification in preparation for \lsst\ and \romantel\ operations.

\section{Objectives}
\label{sec:objectives}

To address these research questions, this thesis pursues the following concrete objectives:

\begin{enumerate}
    \item \textbf{Data Generation and Curation:} Generate a large-scale, high-fidelity synthetic training dataset consisting of over 100,000 simulated PSPL and binary microlensing events. The dataset will systematically vary key physical parameters including mass ratio, projected separation, impact parameter, and Einstein crossing time to ensure comprehensive coverage of the parameter space. Observational effects such as photometric noise, irregular sampling, blending, and finite-source effects will be incorporated to mimic the characteristics of OGLE, MOA, and \lsst\ observations.
    
    \item \textbf{Architecture Development:} Design and implement a novel 1D \cnn\ with a TimeDistributed wrapper specifically tailored for sequential microlensing light curve classification. This architecture will process light curve observations as they arrive in temporal windows, enabling dynamic classification based on the sequence history rather than a single aggregated feature vector. The design will include convolutional filters to extract local temporal patterns, TimeDistributed layers to maintain temporal information, and appropriate regularization techniques (dropout, batch normalization) to prevent overfitting.
    
    \item \textbf{Systematic Performance Evaluation:} Evaluate the developed model's classification performance across multiple dimensions. We will compute accuracy, precision, recall, $F_1$-score, and AUC across a spectrum of observation completeness levels ranging from 10\% to 100\% of the full light curve duration. This systematic evaluation establishes the early detection capability of the model and quantifies the confidence-timeliness trade-off essential for operational deployment.
    
    \item \textbf{Benchmarking and Validation:} Benchmark the TimeDistributed \cnn\ framework against established traditional classification techniques (including $\chi^2$ template fitting following standard practices) and modern machine learning baselines such as feature-based random forests, standard \cnn s without TimeDistributed layers, and recurrent neural networks. We will conduct ablation studies to understand which architectural components contribute most significantly to performance and identify scenarios where each approach excels.
    
    \item \textbf{Framework Finalization and Survey Readiness:} Develop a complete, production-ready classification system designed for deployment in real-time survey operations. This system will include data preprocessing pipelines that handle heterogeneous input formats, quality filtering, normalization schemes optimized for microlensing photometry, model serving infrastructure capable of processing thousands of events per night, and monitoring tools to detect performance degradation. The framework will be designed with modularity to facilitate adaptation to different survey characteristics and straightforward updates as improved models become available.
\end{enumerate}

Together, these objectives aim to deliver both a comprehensive scientific evaluation of early microlensing classification and actionable insights for the design of future survey pipelines, with particular emphasis on \lsst\ and \romantel\ operations.

\section{Contributions}
\label{sec:contributions}

This thesis makes the following contributions to automated microlensing classification:

\begin{enumerate}
    \item \textbf{Large-Scale Synthetic Dataset with Strategic Parameter Sampling:} We generate one million high-fidelity synthetic light curves (500,000 PSPL + 500,000 binary events) using VBMicrolensing for accurate magnification calculations. Binary parameters are strategically sampled to produce events with multiple caustic crossings and distinctive morphological features, establishing an upper bound on classification performance under ideal conditions. The dataset incorporates survey-specific observational effects including adaptable cadence patterns and photometric noise levels calibrated to OGLE (ground-based, high-cadence), \lsst\ (ground-based, multi-band, variable cadence), and \romantel\ (space-based, continuous monitoring) characteristics.
    
    \item \textbf{Systematic Multi-Dimensional Performance Evaluation:} We conduct comprehensive benchmarking across three critical dimensions: observation completeness (10\%--100\%), survey characteristics (three distinct cadence and noise regimes), and binary system parameters (separation, mass ratio, source size). This systematic evaluation quantifies the fundamental trade-offs between classification timeliness, accuracy, and observational constraints, providing concrete operational guidance for next-generation surveys.
    
    \item \textbf{Comparative Analysis with Traditional Methods:} We implement and rigorously compare deep learning approaches against established $\chi^2$ template fitting methods under controlled conditions. This direct comparison, conducted on identical datasets and evaluation protocols, identifies complementary strengths of each approach and scenarios where hybrid strategies may be optimal. We quantify computational efficiency differences critical for real-time survey operations processing thousands of events nightly.
    
    \item \textbf{Survey-Ready Framework with Operational Recommendations:} We deliver a complete, production-tested pipeline encompassing data simulation, model training, and deployment infrastructure designed for integration with \lsst\ and \romantel\ alert streams. The framework includes specific recommendations for trigger thresholds, computational resource allocation, and performance monitoring tailored to each survey's operational constraints. All code and trained models are released open-source to facilitate community adoption and extension.
    
    \item \textbf{Establishment of Performance Baselines for Early Classification:} Through evaluation on the largest microlensing classification dataset assembled to date, we establish definitive performance benchmarks for binary event identification at various completeness levels. These baselines provide reference standards for future algorithm development and enable informed decision-making about when machine learning approaches are preferable to traditional methods in operational contexts.
\end{enumerate}

Collectively, these contributions bridge the gap between proof-of-concept machine learning demonstrations and operational deployment requirements, providing both scientific insights into classification limits and practical tools for next-generation microlensing surveys.

Collectively, these contributions advance the state of the art in automated microlensing classification and provide a blueprint for harnessing deep learning in the era of large-scale time-domain surveys. Beyond microlensing, the techniques developed here may be applicable to other astronomical transients where early identification is crucial, including supernovae, tidal disruption events, and fast radio burst optical counterparts.

\section{Thesis Organization}
\label{sec:organization}

The remainder of this thesis is organized to build progressively from theoretical foundations through implementation to results and implications. Each chapter is designed to be largely self-contained while contributing to the overall narrative:

\textbf{\Cref{ch:theory}: Theoretical Background} provides comprehensive context for understanding gravitational microlensing and the binary classification problem. We begin with a review of gravitational lensing theory, deriving the lens equation and describing the magnification patterns produced by point-mass lenses. We then introduce binary lens systems in detail, explaining the origin of caustic structures and critical curves, and discussing the degeneracies and parameter space complexities that make binary lens modeling challenging. The mathematical formulations for both PSPL and binary lens magnification are presented, with emphasis on the physical interpretation of light curve features. The chapter surveys the operational strategies of key microlensing surveys (OGLE, MOA, \romantel, \lsst) and their typical data products, and reviews the historical development of classification approaches including $\chi^2$ fitting, Bayesian methods, and feature-based classifiers. We conclude by examining recent applications of machine learning to astronomical time-series analysis, with particular attention to deep learning methods for transient classification, and identify the existing gap in sequential modeling for early-time microlensing classification that motivates this work.

\textbf{\Cref{ch:literature}: Literature Review} provides a comprehensive survey of related work across multiple domains. We review the history and current state of microlensing surveys, focusing on detection strategies, data pipelines, and classification challenges. Traditional modeling approaches, including $\chi^2$ fitting and Bayesian inference methods, are examined in detail with emphasis on their computational costs and observational requirements. We survey machine learning applications in time-domain astronomy, covering both classical methods (random forests, support vector machines) and recent deep learning approaches (recurrent neural networks, convolutional architectures). Special attention is given to early classification problems in other astronomical domains, such as supernova typing and transient identification, where similar temporal challenges arise. We conclude by identifying the specific gap that motivates this work: the absence of sequential modeling approaches designed explicitly for early-time microlensing classification.

\textbf{\Cref{ch:methodology}: Methodology} details the technical approach taken to achieve the research objectives. This chapter begins with a comprehensive description of the synthetic data generation pipeline, including the physical models used to simulate both PSPL and binary lens events (lens equations, magnification calculations, and parameter sampling), the parameter ranges and distributions chosen to represent realistic survey detections, and the systematic addition of observational effects such as photometric errors following realistic noise models, cadence variations mimicking OGLE and \lsst, source blending, and finite-source effects. We then present the \cnn\ architecture in detail, explaining the motivation for the TimeDistributed design, the specific layer configurations, activation functions, and regularization strategies employed. The preprocessing steps required to handle irregular sampling and normalize flux measurements are described. We document the training procedures including data augmentation techniques, optimization algorithms, learning rate schedules, and hyperparameter optimization methods. The chapter also describes the implementation of baseline methods for comparison, including $\chi^2$ fitting procedures and feature-based random forest classifiers following established methodologies. We conclude with a detailed description of the evaluation framework, defining all performance metrics and validation strategies.

\textbf{\Cref{ch:results}: Results} presents the core experimental findings. We begin by characterizing the synthetic dataset, showing representative PSPL and binary light curves across the parameter space and validating that the simulations capture realistic event characteristics. We then present classification performance results, with particular focus on accuracy as a function of light curve completeness, demonstrating the early detection capability of the \cnn\ approach through learning curves and performance plateaus. Confusion matrices reveal which types of events are most prone to misclassification, and we investigate how physical parameters (mass ratio, separation, impact parameter) correlate with classification difficulty. The systematic comparison with baseline methods quantifies differences in accuracy, precision, recall, $F_1$-score, and AUC across completeness levels, and examines computational cost trade-offs. We present ablation studies showing the contribution of the TimeDistributed architecture and other design choices. Attention weight visualizations and learned feature analyses provide insight into what patterns the \cnn\ has learned to recognize. We test generalization by varying cadence and noise beyond training distributions, and demonstrate operational performance by applying the framework to a carefully curated set of real OGLE events with known classifications.

\textbf{\Cref{ch:discussion}: Discussion} interprets our findings in the broader context of microlensing science and machine learning applications in astronomy. We discuss the astrophysical implications of early binary detection for survey strategy and follow-up planning, including concrete recommendations for resource allocation in \lsst\ and \romantel\ operations and coordination with ground-based follow-up networks. We analyze the limitations of our approach systematically, examining potential failure modes, assumptions in the training data that may not reflect all real-world scenarios (such as highly asymmetric source crossings or extreme mass ratios), and the challenge of maintaining performance as surveys evolve and new types of events are discovered. We discuss what the interpretability results reveal about the nature of binary lens signatures in incomplete light curves and how these insights relate to traditional physical understanding. The results are contextualized relative to related work in astronomical time-series classification, and we discuss the generalizability of the TimeDistributed architecture to other problems in time-domain astronomy. We identify specific opportunities for future improvements, including incorporation of multi-band color information, extension to triple and higher-order lens systems, integration with anomaly detection frameworks to identify unusual events, and potential for transfer learning to related transient phenomena.

\textbf{\Cref{ch:conclusions}: Conclusions and Future Work} summarizes the key findings and their significance. We systematically review how our work addresses each research question posed in \cref{sec:questions} and fulfills each stated objective from \cref{sec:objectives}. We discuss the readiness of the \cnn\ framework for operational deployment and provide specific, actionable recommendations for survey teams preparing for \lsst\ and \romantel\ operations. We outline several directions for future research, including extensions of the methodology to other microlensing analysis tasks such as real-time detection of finite-source effects and measurement of parallax signatures, applications of the TimeDistributed \cnn\ architecture to other classes of astronomical transients including supernovae and tidal disruption events, potential for transfer learning from microlensing to gravitational wave electromagnetic counterpart classification, and strategies for continual learning as new data become available. We conclude with reflections on the evolving role of machine learning in modern astronomical surveys, emphasizing the importance of developing interpretable, robust, and scientifically validated deep learning tools that complement rather than replace physical understanding.

\textbf{Appendices} provide essential supporting technical material. Appendix A presents detailed derivations of microlensing magnification formulas for both PSPL and binary lenses, including edge cases and limiting behaviors. Appendix B documents the synthetic data generation code, parameter sampling procedures, and validation tests. Appendix C provides the complete \cnn\ architecture specifications, layer configurations, and training hyperparameters for full reproducibility. Appendix D presents additional performance metrics, confusion matrices for parameter subsets, and supplementary figures characterizing model behavior. Appendix E discusses the computational infrastructure used for training and evaluation, including hardware specifications, parallelization strategies, runtime performance measurements, and estimates of computational requirements for \lsst-scale deployment.

This organizational structure ensures that readers can follow the logical progression from motivation through methods to conclusions, while also enabling experts to navigate directly to chapters of specific interest. Figures and tables throughout illustrate key concepts, simulation outcomes, and classifier performance. A comprehensive bibliography provides context within the broader literature of microlensing physics, exoplanet detection, and machine learning applications in astronomy.