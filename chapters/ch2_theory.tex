\chapter{Theoretical Background}
\label{ch:theory}

This chapter establishes the theoretical foundation for gravitational microlensing and the classification problem addressed in this thesis. We begin with the general relativistic basis of gravitational lensing (\cref{sec:grav_lensing}), derive the lens equation and magnification formulas for point-source point-lens (PSPL) events (\cref{sec:pspl_model}), introduce binary lens systems with their characteristic caustic and critical curve structures (\cref{sec:binary_lensing}), and conclude with a minimal introduction to the machine learning approach employed (\cref{sec:ml_overview}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gravitational Lensing}
\label{sec:grav_lensing}

\subsection{Historical Foundation}
\label{sec:history}

The phenomenon of light deflection by massive objects represents one of the most striking predictions of Einstein's general theory of relativity. In 1915, Einstein demonstrated that spacetime curvature caused by mass would bend the paths of light rays passing nearby. This prediction was famously confirmed during the solar eclipse of 1919, when Arthur Eddington observed the apparent displacement of stars near the Sun's limb \citep{Dyson1920}. The measured deflection of approximately 1.75 arcseconds agreed closely with Einstein's prediction and was twice the value expected from Newtonian gravity. This result, widely publicized in the press, made Einstein a household name and inaugurated the study of gravitational lensing.

While early observations focused on strong lensing effects by galaxies---producing multiple resolved images and dramatic Einstein rings---gravitational microlensing operates on much smaller angular scales. Rather than resolving separate images, microlensing by individual stars manifests as temporary brightness variations in background sources. The image separations are of order milli-arcseconds, too small to be resolved with current instruments; instead, microlensing is detected through time-varying brightness changes.

\subsection{The Lens Equation}
\label{sec:lens_equation}

Consider a light ray passing a lens of mass $M$ at impact parameter $\xi$ (measured perpendicular to the unperturbed ray direction). In the thin-lens approximation, appropriate when lens thickness is small compared to source-lens and lens-observer distances, the deflection angle is:
\begin{equation}
\hat{\alpha}({\bm \xi}) = \frac{4GM}{c^2} \frac{\bm \xi}{|\bm \xi|^2},
\label{eq:deflection}
\end{equation}
where $G$ is the gravitational constant and $c$ is the speed of light. This deflection creates an apparent displacement between the source's true position ${\bm \beta}$ and its observed position ${\bm \theta}$ in the sky.

The geometry of lensing involves three distances: $D_{\mathrm{L}}$ (observer to lens), $D_{\mathrm{S}}$ (observer to source), and $D_{\mathrm{LS}} = D_{\mathrm{S}} - D_{\mathrm{L}}$ (lens to source). Angular positions in the source plane ${\bm \beta}$ and lens plane ${\bm \theta}$ are related through the lens equation:
\begin{equation}
{\bm \beta} = {\bm \theta} - \frac{D_{\mathrm{LS}}}{D_{\mathrm{S}}} \hat{\bm \alpha}\!\left(D_{\mathrm{L}}\,{\bm \theta}\right).
\label{eq:lens_equation}
\end{equation}
This vector equation represents the requirement that light rays from the source position ${\bm \beta}$ must deflect to reach the observer at apparent position ${\bm \theta}$.

\subsection{The Microlensing Regime}
\label{sec:microlensing_regime}

Microlensing is characterized by two defining properties that distinguish it from strong lensing by galaxies. First, the angular separations between multiple images are extremely small---of order microarcseconds to milliarcseconds---preventing their resolution with current instrumentation. Second, the lens-source relative motion is measurable on human timescales (days to months), producing time-variable magnification as the alignment changes. These properties emerge when the lens mass is stellar-scale ($M \sim 0.01\text{--}10\,M_{\odot}$) and source-lens-observer distances are kiloparsecs.

A natural angular scale for the problem is the Einstein radius $\theta_E$, defined as the angular radius of the Einstein ring produced when source and lens are perfectly aligned:
\begin{equation}
\theta_E = \sqrt{\frac{4GM}{c^2} \frac{D_{\mathrm{LS}}}{D_{\mathrm{L}} D_{\mathrm{S}}}}.
\label{eq:einstein_radius}
\end{equation}
For typical Galactic microlensing geometry---a solar-mass lens at 4~kpc lensing a source at 8~kpc in the bulge---$\theta_E \approx 0.5$ milliarcseconds. Working in units of $\theta_E$ simplifies the equations considerably.

The physical size corresponding to $\theta_E$ projected onto the lens plane defines the Einstein radius proper:
\begin{equation}
r_E = D_{\mathrm{L}}\,\theta_E = \sqrt{\frac{4GM\,D_{\mathrm{L}}\,D_{\mathrm{LS}}}{c^2\,D_{\mathrm{S}}}}.
\end{equation}
For solar-mass lenses in the bulge, $r_E \sim 3$ AU. This scale is comparable to planetary orbital separations, explaining why microlensing is sensitive to planetary companions around the lens star.

The Einstein crossing time $t_E$ is the time required for the source to traverse an Einstein radius due to lens-source relative transverse motion:
\begin{equation}
t_E = \frac{r_E}{v_{\perp}} = \frac{\theta_E\,D_{\mathrm{S}}}{v_{\perp}},
\end{equation}
where $v_{\perp}$ is the lens-source transverse velocity. For typical Galactic kinematics with $v_{\perp} \sim 200$ km/s, Einstein crossing times range from 10 to 100 days. This timescale determines the duration over which a microlensing event is observable and sets the required survey cadence for adequate temporal sampling.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Point-Source Point-Lens Model}
\label{sec:pspl_model}

The simplest microlensing configuration involves a point-mass lens and a point source. Despite its simplicity, this model describes the majority of detected events and provides the baseline against which binary lens events are distinguished.

\subsection{Magnification Pattern}
\label{sec:pspl_magnification}

For a point-mass lens, \cref{eq:lens_equation} admits exactly two solutions corresponding to two images, one inside and one outside the Einstein ring. The magnification of each image is determined by the local Jacobian of the lens mapping, yielding a total magnification:
\begin{equation}
A_{\text{PSPL}}(u) = \frac{u^2 + 2}{u\sqrt{u^2 + 4}},
\label{eq:pspl_mag}
\end{equation}
where $u$ is the instantaneous lens-source separation in units of $\theta_E$. The magnification diverges as $u \to 0$ (perfect alignment) and approaches unity as $u \to \infty$ (no lensing effect).

For a source moving with constant transverse velocity, the separation evolves as:
\begin{equation}
u(t) = \sqrt{u_0^2 + \left(\frac{t - t_0}{t_E}\right)^2},
\label{eq:u_evolution}
\end{equation}
where $u_0$ is the impact parameter (minimum separation at time $t_0$). The resulting light curve $A(t)$ exhibits the characteristic symmetric, achromatic magnification peak that defines PSPL events. High-magnification events ($u_0 \ll 1$) show dramatic flux increases, while low-magnification events ($u_0 \gtrsim 1$) produce only subtle brightening.

\subsection{Observable Features}
\label{sec:pspl_features}

PSPL light curves are fully determined by three parameters: the Einstein crossing time $t_E$, the impact parameter $u_0$, and the time of closest approach $t_0$. These parameters govern the event duration, maximum magnification, and temporal position respectively. Importantly, PSPL light curves are perfectly symmetric about $t_0$---the rise and fall phases are mirror images. They are also strictly achromatic: the magnification factor is independent of wavelength since it depends only on geometry and mass, not on the intrinsic spectral energy distribution of either lens or source. These properties---three-parameter simplicity, temporal symmetry, and achromaticity---make PSPL events straightforward to model and serve as the reference template for identifying deviations caused by binary lens systems.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter2/pspl_geometry.pdf}
    \caption[Point-source point-lens geometry and light curve]{Point-source point-lens geometry and light curve. Left: Source trajectory (blue line) with impact parameter $u_0$ relative to the lens (black dot). Right: Resulting symmetric magnification pattern showing characteristic temporal profile with Einstein timescale $t_E$.}
    \label{fig:pspl_geometry}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Binary Lensing}
\label{sec:binary_lensing}

Binary lens systems---composed of two masses such as a host star with a planetary companion or a binary star pair---produce substantially more complex magnification patterns than single lenses. The richness of binary lens phenomenology arises from caustic and critical curve structures that create regions of extreme magnification and sharp temporal features in the light curves.

\subsection{The Binary Lens Equation}
\label{sec:binary_equation}

For a binary system with mass ratio $q = M_2/M_1$ and separation $s$ (in Einstein radius units), the lens equation generalizes to:
\begin{equation}
{\bm \beta} = {\bm \theta} - \frac{1}{1+q}\frac{{\bm \theta} - {\bm \theta}_1}{|{\bm \theta} - {\bm \theta}_1|^2} - \frac{q}{1+q}\frac{{\bm \theta} - {\bm \theta}_2}{|{\bm \theta} - {\bm \theta}_2|^2},
\label{eq:binary_lens_equation}
\end{equation}
where ${\bm \theta}_1$ and ${\bm \theta}_2$ are the positions of the two lens components. Unlike the single-lens case, this equation cannot be solved analytically; it admits up to five image solutions depending on the source position, and numerical methods are required for evaluation \citep{Bozza2010}.

The parameter space for binary lensing is significantly richer than PSPL. In addition to $t_E$, $t_0$, and $u_0$, binary events require specification of the mass ratio $q$ (spanning four orders of magnitude from planetary mass ratios $q \sim 10^{-4}$ to equal-mass binaries $q = 1$), the projected separation $s$ (typically 0.3--3.0 Einstein radii), and the source trajectory angle $\alpha$ relative to the binary axis. This high-dimensional parameter space, combined with complex topologies that change qualitatively across the parameter ranges, makes comprehensive binary lens modeling computationally demanding.

\subsection{Caustics and Critical Curves}
\label{sec:caustics}

The defining structures in binary lensing are \textit{caustics} in the source plane and \textit{critical curves} in the image plane. These are intrinsically related: critical curves are locations in the image plane where the lens mapping Jacobian becomes singular (i.e., where det$|\partial {\bm \beta}/\partial {\bm \theta}| = 0$), corresponding to infinite magnification. The caustics are the images of the critical curves under the lens mapping---that is, the locations in the source plane that map to the critical curves in the image plane. Mathematically, if ${\bm \theta}_{\mathrm{crit}}$ is a point on the critical curve, then ${\bm \beta}_{\mathrm{caustic}} = {\bm \theta}_{\mathrm{crit}} - \hat{\bm \alpha}\!\left({\bm \theta}_{\mathrm{crit}}\right)$ is the corresponding point on the caustic.

When a source crosses a caustic, a pair of images is created or destroyed at the critical curve, producing a rapid change in total magnification. This manifests as a sharp spike or cusp in the observed light curve---the characteristic signature that distinguishes binary events from smooth PSPL profiles. The closer a source trajectory passes to a caustic without crossing it, the more pronounced the deviation from PSPL behavior becomes, appearing as asymmetric perturbations in the light curve.

The topology of caustics depends sensitively on the binary parameters $s$ and $q$. For wide binaries ($s \gtrsim 1.0$), the caustic structure typically consists of a central caustic near the system's center of mass and a smaller planetary caustic near the lower-mass component. For close binaries ($s \lesssim 1.0$), the caustics can merge into more complex configurations. For planetary mass ratios ($q \ll 1$), the planetary caustic becomes very small, requiring high-magnification events to probe it, but caustic crossings produce the sharpest spikes that enable planetary mass and separation determinations.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter2/binary_caustics.pdf}
    \caption[Binary lens caustic and critical curve structures]{Binary lens caustic and critical curve structures for different parameter regimes. The caustics (red regions in source plane) represent locations of formally infinite magnification for point sources. Critical curves (blue curves shown in insets) are the corresponding structures in the image plane where image pairs are created or destroyed. The two black dots indicate lens positions (primary at origin, companion at separation $s$). Source trajectories (dashed lines) crossing or approaching caustics produce characteristic deviations from PSPL light curves.}
    \label{fig:binary_caustics}
\end{figure}

\subsection{Light Curve Morphologies}
\label{sec:binary_morphologies}

The magnification for a binary lens must be computed by summing contributions from all images. Since no closed-form solution exists, numerical techniques such as ray-shooting or inverse ray-tracing are employed at each time step \citep{Bozza2010}. The resulting light curves exhibit several characteristic features that distinguish them from PSPL events:

\begin{itemize}
    \item \textbf{Caustic crossings:} When the source trajectory intersects a caustic, the light curve displays a sharp spike or cusp. The spike duration depends on the source size (for finite sources) and the transverse velocity. For point sources, caustic crossings would produce mathematically infinite peaks; realistic finite source sizes ($\rho \sim 10^{-3}$ to $10^{-2}$ Einstein radii) smooth these features into peaks with characteristic widths determined by $\rho$ and the crossing speed.
    
    \item \textbf{Asymmetric profiles:} Unlike the perfect symmetry of PSPL curves, binary light curves typically show asymmetric rise and fall times. The shape depends on the source trajectory relative to the caustic structure and the binary orientation angle $\alpha$.
    
    \item \textbf{Multiple peaks:} Depending on geometry, a source may cross multiple caustic features or approach several caustics without crossing, producing light curves with two or more distinct peaks separated by days to weeks.
    
    \item \textbf{Extended perturbations:} Even without caustic crossings, the perturbation from the binary companion creates extended deviations from the PSPL template. Near-caustic approaches produce asymmetric bumps or shoulders in the light curve that can last for a significant fraction of the Einstein timescale.
\end{itemize}

The probability of detecting binary features depends critically on the impact parameter $u_0$. High-magnification events ($u_0 \lesssim 0.3$) have source trajectories that pass close to or through caustic structures, making binary signatures prominent and easily distinguishable from PSPL profiles. Low-magnification events ($u_0 \gtrsim 0.3$), however, have trajectories that may never approach caustics closely, and binary perturbations become subtle or absent. This fundamental geometric constraint means binary lenses are intrinsically indistinguishable from single lenses in certain parameter regimes---a physical detection limit rather than an algorithmic limitation.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter2/binary_lightcurves.pdf}
    \caption[Binary microlensing light curve morphologies]{Binary microlensing light curve morphologies and their caustic geometries. Each row shows a source trajectory (left) and the resulting light curve (right) for different binary parameters and impact parameters. Top: Caustic crossing produces sharp flux spike. Middle: Near-caustic approach creates asymmetric perturbation. Bottom: Distant trajectory yields nearly PSPL-like profile, illustrating the fundamental detection limit at large impact parameters.}
    \label{fig:binary_lightcurves}
\end{figure}

\subsection{Planetary Companions}
\label{sec:planetary}

Planetary companions with mass ratios $q \sim 10^{-3}$ to $10^{-4}$ produce small planetary caustics with characteristic sizes proportional to $\sqrt{q}$. The probability of a random source trajectory crossing these small caustics is low, but when crossings occur, they produce short-duration ($\Delta t \sim 0.01\text{--}0.1\,t_E$), high-amplitude spikes that are unmistakable signatures of planetary companions. These spikes can occur at any phase of the event---during the initial rise, near the peak, or during the decline---depending on the planetary separation and the orientation of the source trajectory.

High-cadence observations (multiple times per night) are essential to capture these brief planetary signals, which might be missed by surveys with daily sampling. The detection of such features enables measurements of the planet-star mass ratio and projected separation with precision sufficient to constrain planetary formation scenarios and contribute to demographic studies of exoplanet populations across the Galaxy.

The complexity of binary lens modeling---with its seven-plus free parameters, multiply-peaked likelihood surfaces, and computationally expensive magnification calculations---motivates the machine learning approach developed in this thesis. Traditional $\chi^2$ fitting methods become prohibitively slow when analyzing thousands of ongoing events in real time, particularly when attempting early classification before the full light curve morphology is revealed.

% ============================================================================
% ENHANCED CHAPTER 2 - MACHINE LEARNING SECTION
% Replace lines 165-204 in ch2_theory_FINAL_CORRECTED.tex with this
% ============================================================================

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Machine Learning for Microlensing Classification}
\label{sec:ml_overview}

The application of machine learning to gravitational microlensing presents an opportunity to automate the identification of rare binary events within large datasets generated by current and next-generation surveys. Rather than relying on human inspection or computationally expensive model fitting, supervised classification methods can learn to recognize distinctive patterns in partially observed light curves, enabling real-time event prioritization.

\subsection{The Classification Task}
\label{sec:classification_task}

The problem addressed in this work is supervised classification: given a partially observed light curve (flux measurements at discrete time points), assign it to one of three categories---flat (no lensing event), PSPL (single-lens event), or binary (binary-lens event). The classifier must provide a probability distribution over these three classes, enabling downstream decision-making about resource allocation for follow-up observations \citep{Lochner2016}. Critically, the classifier must operate in real time, updating its classification as new observations arrive, using only past and present data.

This differs fundamentally from traditional model fitting approaches that attempt to estimate the continuous physical parameters ($t_E$, $u_0$, $q$, $s$, etc.). Instead of solving an inverse problem to recover lens properties, the classifier performs pattern recognition, identifying which class of magnification pattern best describes the observed light curve evolution. The classifier does not explicitly compute caustic positions, solve lens equations, or perform numerical ray-tracing; rather, it learns to recognize the temporal signatures that distinguish binary caustic crossings from PSPL symmetry through exposure to large numbers of training examples.

\subsection{Neural Networks for Time Series}
\label{sec:nn_timeseries}

Neural networks are computational models composed of layers of parameterized functions (neurons) that transform input data through successive nonlinear operations \citep{Goodfellow2016}. Through a process called training, the network parameters (weights and biases) are adjusted via backpropagation and gradient descent to minimize prediction errors on labeled examples \citep{Rumelhart1986}. Modern deep learning architectures with multiple hidden layers can learn hierarchical representations, extracting increasingly abstract features from raw input data \citep{LeCun2015}. For time-series classification tasks like microlensing, architectures specifically designed to handle sequential data have proven particularly effective \citep{VanderPlas2018,Ismail2019}.

\textit{Convolutional Neural Networks} (CNNs) apply learnable filters that slide across the input sequence, detecting local patterns such as the rapid flux changes characteristic of caustic crossings \citep{LeCun1998,Krizhevsky2012}. Convolutional layers are translation-invariant, meaning they can recognize features regardless of where they occur in the time series. Multiple convolutional layers with different filter sizes enable the network to detect patterns at multiple temporal scales, from the sharp sub-day features of planetary caustic crossings to the weeks-long evolution of the overall magnification envelope \citep{Shallue2018}. Pooling operations following convolutional layers provide spatial downsampling while preserving the most salient features, improving computational efficiency and model robustness.

\textit{Recurrent Neural Networks} (RNNs) maintain an internal hidden state that evolves as they process each observation sequentially, allowing them to remember features from earlier time steps and integrate information across the full sequence \citep{Lipton2015,Naul2018}. The hidden state provides a form of memory that captures the cumulative context of the light curve as it develops. Gated variants such as Long Short-Term Memory (LSTM) units \citep{Hochreiter1997} and Gated Recurrent Units (GRUs) \citep{Cho2014,Chung2014} use learned gating mechanisms to selectively retain or forget information. These gates enable the network to capture both short-term patterns---such as a recent caustic crossing---and long-term structure---such as the overall symmetric or asymmetric shape of the light curve \citep{Moller2020}. GRUs offer computational advantages over LSTMs while maintaining comparable representational capacity, making them well-suited for real-time inference applications \citep{Chung2014}.

The architecture employed in this work combines both approaches: convolutional layers extract local temporal features from the raw flux measurements, while recurrent GRU layers integrate these features across the full observation sequence, producing a final representation that is classified into one of the three event categories. This hierarchical structure mirrors the logical organization of the astrophysical problem---first detecting any lensing signal (distinguishing flat from non-flat), then distinguishing single from binary configurations based on the presence of caustic crossing signatures. The combination of CNN feature extraction with RNN sequence modeling has proven highly effective for astronomical time series classification across multiple domains \citep{Naul2018,Shallue2018,VanderPlas2018}.

\subsection{Training on Synthetic Data}
\label{sec:synthetic_training}

A critical advantage of machine learning for microlensing is that training data can be generated synthetically with perfect ground truth labels. Using physics-based simulation codes such as VBBinaryLensing \citep{Bozza2010}, we can generate unlimited light curves spanning the full parameter space of lens masses, separations, trajectories, and observational conditions. Each synthetic event is labeled with its true class (flat, PSPL, or binary), providing the supervised training signal required for the network to learn discriminative features. This approach circumvents the label scarcity problem that limits many astronomical machine learning applications where labeled training sets must be painstakingly curated by human experts \citep{Lochner2016}.

Synthetic training data enables systematic exploration of rare configurations and boundary cases that may be underrepresented in observed datasets. For planetary mass ratios $q \sim 10^{-4}$, caustic features become extremely small, requiring close source-lens alignments ($u_0 \ll 0.1$) for detection. By deliberately oversampling these challenging regimes during training, the network can learn to recognize subtle signatures that would be difficult to identify through traditional parameter fitting. The use of synthetic data also allows controlled studies of classifier performance as a function of observational quality, temporal sampling, and photometric precision \citep{Godines2019,Khakpash2021}.

The challenge lies in ensuring that synthetic training data accurately represents the statistical properties of real survey populations. Parameter sampling strategies must balance two competing objectives: generating enough examples with clear binary signatures to enable the network to learn caustic crossing patterns, while also including the realistic mix of high-impact-parameter events where binary features are subtle or absent. Domain adaptation techniques and transfer learning approaches \citep{Weiss2016} can help address potential distribution shifts between synthetic training data and real observations, though validation on actual survey data remains essential for operational deployment.

\subsection{Optimization and Regularization}
\label{sec:optimization}

Training neural networks involves minimizing a loss function that measures prediction error on the training set. For classification tasks, the cross-entropy loss between predicted class probabilities and true labels provides the objective function. Optimization proceeds through stochastic gradient descent (SGD) and its variants, where network parameters are iteratively updated based on gradients computed via backpropagation \citep{Rumelhart1986}. Modern adaptive optimizers such as Adam \citep{Kingma2014} and AdamW \citep{Loshchilov2019} adjust learning rates on a per-parameter basis, accelerating convergence while improving generalization.

Regularization techniques prevent overfitting---where the network memorizes training examples rather than learning generalizable patterns. Dropout \citep{Srivastava2014} randomly deactivates neurons during training, forcing the network to learn redundant representations and improving robustness. Batch normalization \citep{Ioffe2015} standardizes layer inputs, stabilizing training and acting as an implicit regularizer. Weight decay (L2 regularization) penalizes large parameter values, encouraging simpler solutions that generalize better to unseen data. The methodology for hyperparameter selection and training procedures is detailed in \cref{ch:methodology}.

\subsection{Real-Time Processing and Calibration}
\label{sec:realtime}

Operational deployment in survey pipelines requires sub-millisecond inference times to process thousands of ongoing events as observations arrive. Modern GPU-accelerated neural network inference can achieve this performance through efficient matrix operations and optimized implementations \citep{Raina2009,Chetlur2014}. Unlike traditional fitting methods that require iterative optimization---often taking seconds to minutes per evaluation for complex binary models---neural network inference is a single forward pass through the network, providing a fixed computational cost independent of event complexity. This enables truly real-time classification as each new observation is obtained.

The classifier outputs continuously updated probabilities $P(\text{Flat})$, $P(\text{PSPL})$, and $P(\text{Binary})$ that evolve as the light curve develops. These probabilities can guide automated decisions: events with rising $P(\text{Binary})$ might trigger increased observing cadence or alert ground-based follow-up networks, while events with high $P(\text{Flat})$ can be deprioritized to conserve resources. Properly calibrated probability estimates---where predicted confidence levels match true classification accuracy \citep{Guo2017}---are essential for reliable operational decision-making. Calibration can be assessed through reliability diagrams and expected calibration error metrics, ensuring that when the classifier reports 90\% confidence, it is indeed correct 90\% of the time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary}
\label{sec:theory_summary}

Gravitational microlensing provides a unique window into Galactic populations of planets, stars, and stellar remnants through its mass-dependent sensitivity. Point-source point-lens events produce characteristic symmetric, three-parameter light curves that serve as the baseline template for microlensing observations. Binary lens systems introduce caustic and critical curve structures that create sharp temporal features and asymmetric patterns, enabling planetary detections and mass measurements but dramatically increasing the parameter space complexity.

The classification problem---distinguishing binary from single-lens events using partial observations---is well-suited to machine learning approaches that can learn to recognize caustic crossing signatures from large training datasets. The combination of convolutional feature extraction for local patterns and recurrent sequence modeling for long-term memory provides an architecture capable of real-time classification as observations arrive. Training on synthetic data generated with physics-based simulations enables comprehensive coverage of the parameter space while providing perfect ground truth labels.

The following chapters present the implementation of this approach, beginning with the synthetic data generation pipeline and neural network architecture (\cref{ch:methodology}), proceeding to experimental results and validation across different population distributions (\cref{ch:results}), and concluding with physical interpretation of the classification boundaries and operational implications for next-generation surveys (\cref{ch:discussion}).